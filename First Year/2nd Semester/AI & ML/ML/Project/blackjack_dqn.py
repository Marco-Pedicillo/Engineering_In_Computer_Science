# -*- coding: utf-8 -*-
"""BlackJack_DQN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b44xaQjWj-B5IJdbyXi6A8UDBVp4qjWJ



# **Imports and Environment Setup**

This code includes all necessary imports and initializes the Blackjack environment from the Gymnasium library
"""

#!pip install gymnasium torch numpy matplotlib --quiet

import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque
from collections import Counter
import matplotlib.pyplot as plt

# Create the Blackjack environment
# 'sab=True' enables the version of Blackjack used in the Sutton & Barto textbook
env = gym.make("Blackjack-v1", sab=True)

"""# **DQN Setup**

This code defines the core components of a DQN Agent. 
It sets up the main training hyperparameters and defines the neural network architecture used to approximate the Q-function, as well as the replay buffer, 
a memory module that stores past experiences and allows the agent to train using randomly sampled batches.
"""

# Hyperparameters
episodes = 600000           # Total number of training episodes
gamma = 0.99                # Discount factor for future rewards
epsilon_start = 1.0         # Initial epsilon value (100% exploration)
epsilon_end = 0.05          # Minimum epsilon value (5% exploration)
epsilon_decay = 80000       # Rate of exponential epsilon decay
lr = 0.001                  # Learning rate for optimizer
batch_size = 64             # Mini-batch size sampled from replay buffer
buffer_size = 20000         # Maximum capacity of replay buffer
min_buffer_size = 2000      # Minimum buffer size before training begins
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Use GPU if available, otherwise fallback to CPU


# Exponential decay of epsilon over time (used for Œµ-greedy policy)
def get_epsilon(frame_idx):
    return epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * frame_idx / epsilon_decay)

# Deep Q-Network architecture -> Q(s,a) ‚âà rete¬†neurale(s)[a]
class DQN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(3, 64),   # Input: 3 features (player sum, dealer card, usable ace)
            nn.ReLU(),          # Activation
            nn.Linear(64, 64),  # Hidden layer
            nn.ReLU(),          # Activation
            nn.Linear(64, 2)    # Output: Q-values for 2 actions (stick, hit)
        )
        # The network tells you how good each action is given a state

    def forward(self, x):
        return self.fc(x)

# Replay Buffer for experience replay
class ReplayBuffer:
    def __init__(self, capacity):
        # Use a double-ended queue to store transitions with a fixed maximum size
        self.buffer = deque(maxlen=capacity)

    # Add a single transition (experience) to the buffer
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    # Randomly sample a batch of experiences from the buffer
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        # Convert to PyTorch tensors and move to device (CPU or GPU)
        return (
            torch.FloatTensor(states).to(device),
            torch.LongTensor(actions).to(device),
            torch.FloatTensor(rewards).to(device),
            torch.FloatTensor(next_states).to(device),
            torch.FloatTensor(dones).to(device)
        )

    # Return the current number of stored transitions
    def __len__(self):
        return len(self.buffer)

"""# **DQN Training & Performance**

This code implements the training for a DQN Agent. It includes the Œµ-greedy policy for action selection, experience storage in a replay buffer, 
and training the neural network with sampled mini-batches. The agent learns over 600.000 episodes, and its performance is tracked. 
This structure demonstrates how the agent improves over time.
"""

# Initialization
model = DQN().to(device)  # Create the Q-network and move it to CPU or GPU
optimizer = optim.Adam(model.parameters(), lr=lr)  # Use Adam optimizer with specified learning rate
loss_fn = nn.MSELoss()  # Mean Squared Error loss function for Q-learning target updates
buffer = ReplayBuffer(buffer_size)  # Create the replay buffer with specified capacity
episode_rewards = []  # To store total reward per episode
step_count = 0  # Used to compute epsilon decay over time

# Training over episodes
for episode in range(episodes):
    state, _ = env.reset()  # Reset the environment to start a new episode
    done = False
    total_reward = 0  # Cumulative reward for this episode

    while not done:
        # Convert current state to a tensor for input to the model
        state_tensor = torch.FloatTensor([state]).to(device)

        # Compute current epsilon based on step count
        epsilon = get_epsilon(step_count)
        step_count += 1

        # Œµ-greedy policy: choose random action with prob Œµ, else use the model
        if random.random() < epsilon:
            action = env.action_space.sample()
        else:
            with torch.no_grad():  # Don't track gradients during inference
                q_values = model(state_tensor)  # Predict Q-values for current state
                action = torch.argmax(q_values).item()  # Select action with highest Q-value

        # Perform action and observe the result
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated  # Check if episode is done (natural end or truncated)
        total_reward += reward  # Accumulate reward

        # Save experience in the replay buffer
        buffer.push(state, action, reward, next_state, done)

        # Train only if we have enough experiences in the buffer
        if len(buffer) >= min_buffer_size:
            # Sample a random batch of transitions
            states, actions, rewards, next_states, dones = buffer.sample(batch_size)

            # Predict Q-values for current states
            q_values = model(states)
            q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # Select Q(s, a)

            # Predict Q-values for next states and take the max over actions
            next_q_values = model(next_states)
            next_q_value = next_q_values.max(1)[0]

            # Compute target: r + Œ≥ * max_a' Q(s', a') if not done
            targets = rewards + gamma * next_q_value * (1 - dones)

            # Compute loss and update model
            loss = loss_fn(q_value, targets.detach())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # Move to the next state
        state = next_state

    # Store total reward for this episode
    episode_rewards.append(total_reward)

    # Print average reward every 2000 episodes
    if (episode + 1) % 5000 == 0:
        avg = np.mean(episode_rewards[-5000:])
        print(f"Episode {episode+1}, Average Reward: {avg:.3f}, Epsilon: {epsilon:.3f}")


# Plot average reward over training episodes
print("\n")
plt.plot(np.convolve(episode_rewards, np.ones(1000)/1000, mode='valid'))
plt.title("Average reward")
plt.xlabel("Episode")
plt.ylabel("Average reward")
plt.grid(True)
plt.show()

# Plot epsilon decay
print("\n")
epsilons = [get_epsilon(i) for i in range(len(episode_rewards))]
plt.figure()
plt.plot(epsilons)
plt.title("Epsilon decay over episodes")
plt.xlabel("Step")
plt.ylabel("Epsilon")
plt.grid(True)
plt.show()

"""# **DQN Evaluation and Reward Analysis**

This block evaluates the performance of a trained Deep Q-Network (DQN) agent by running 10000 episodes in exploitation mode, 
using only the learned policy without exploration. It calculates the average reward across episodes and provides a detailed distribution of outcomes 
(wins, draws, and losses).
"""

def evaluate_agent(model, episodes=10000, return_distribution=False):

    model.eval()
    rewards = []

    for _ in range(episodes):
        state, _ = env.reset()
        done = False
        total_reward = 0

        while not done:
            state_tensor = torch.FloatTensor([state]).to(device)

            with torch.no_grad():
                q_values = model(state_tensor)
                action = torch.argmax(q_values).item()

            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            total_reward += reward
            state = next_state

        rewards.append(total_reward)

    if return_distribution:
        return rewards
    else:
        return np.mean(rewards)


# Results
rewards = evaluate_agent(model, episodes=10000, return_distribution=True)
counts = Counter(rewards)
total = len(rewards)

print("Results over 10000 episodes:")
print(f"  Wins: {counts[1.0]} ({100 * counts[1.0] / total:.2f}%) ‚úÖ ")
print(f"  Losses: {counts[-1.0]} ({100 * counts[-1.0] / total:.2f}%) ‚ùå")
print(f"  Draws: {counts[0.0]} ({100 * counts[0.0] / total:.2f}%) ü§ù")
print(f"  Average reward: {np.mean(rewards):.4f} üèÜ")