# -*- coding: utf-8 -*-
"""BlackJack_Q_Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XxB_dfVbQQR5Wxro1AT43SWoVTaHjgz3

    


# **Imports and Environment Setup**

This code initializes the Blackjack environment from the Gymnasium library. 
We define a done flag that will later be used to control the episode loop â€” this flag indicates whether the current Blackjack game (episode) has ended. 
At this stage, no actions are taken yet: we are just preparing the environment and the control variable for future interaction.
"""

#!pip install gymnasium[classic-control]
# (Only needed once in Colab to install Gymnasium)

from collections import defaultdict # For initializing the Q-table later
import matplotlib.pyplot as plt # For plotting graphs
import numpy as np
import seaborn as sns # For heatmaps
from matplotlib.patches import Patch # For custom legends
from tqdm import tqdm # For progress bars during training
import gymnasium as gym # Main library to create and interact with environments

# Create the Blackjack environment
# 'sab=True' enables the version of Blackjack used in the Sutton & Barto textbook
env = gym.make("Blackjack-v1", sab=True)

done = False # Flag to track whether the current episode has ended

"""# **Agent Definition and Hyperparameters**

In this section, we define the Blackjack Agent class that implements the Q-learning algorithm for solving the Blackjack-v1 environment from Gymnasium. 
The agent uses a tabular representation of Q-values to learn an optimal policy through interaction with the environment. 
It balances exploration and exploitation using an epsilon-greedy strategy, gradually reducing exploration over time. 
We also set the training hyperparameters and instantiate the agent.
"""

class BlackjackAgent:
    # Initialize the agent with a Q-table and key RL parameters.
    def __init__(
        self,
        env,
        learning_rate: float, # How much to update the Q-value at each step
        initial_epsilon: float, # Initial probability of choosing a random action
        epsilon_decay: float, # How much to reduce epsilon after each episode
        final_epsilon: float, # Minimum allowed value for epsilon
        discount_factor: float = 0.95, # Gamma, determines the importance of future rewards
    ):

        # Initialize Q-table with zeros for each possible action
        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))

        self.lr = learning_rate
        self.discount_factor = discount_factor

        self.epsilon = initial_epsilon
        self.epsilon_decay = epsilon_decay
        self.final_epsilon = final_epsilon

        # List to keep track of TD error during training
        self.training_error = []

    def get_action(self, env, obs: tuple[int, int, bool]) -> int:

        # Selects an action using epsilon-greedy policy.
        # With probability epsilon: choose random action (exploration).
        # With probability (1 - epsilon): choose best known action (exploitation).

        if np.random.random() < self.epsilon:
            return env.action_space.sample() # Explore
        else:
            return int(np.argmax(self.q_values[obs])) # Exploit

    def update(
        self,
        obs: tuple[int, int, bool],
        action: int,
        reward: float,
        terminated: bool,
        next_obs: tuple[int, int, bool],
    ):
        # Update the Q-value of the (state, action) pair using the Q-learning update rule.

        # If episode has not ended, consider max Q-value of next state; otherwise 0
        future_q_value = (not terminated) * np.max(self.q_values[next_obs])

        # Compute Temporal Difference (TD) error
        temporal_difference = ( reward + self.discount_factor * future_q_value - self.q_values[obs][action] )

        # Q-learning update step
        self.q_values[obs][action] += self.lr * temporal_difference
        self.training_error.append(temporal_difference)

    def decay_epsilon(self):
        # Reduce epsilon after each episode, keeping it above the minimum (final_epsilon).
        # This ensures the agent explores a lot initially and then gradually exploits.
        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)


# Set key training hyperparameters
learning_rate = 0.01 # Step size for Q-value updates
n_episodes = 600000 # Number of episodes for training
start_epsilon = 1.0 # Start with full exploration
epsilon_decay = start_epsilon / (n_episodes / 2)  # Gradually decrease epsilon
final_epsilon = 0.05 # Minimum exploration allowed

# Create the Blackjack agent using the environment and hyperparameters
agent = BlackjackAgent(
    env=env,
    learning_rate=learning_rate,
    initial_epsilon=start_epsilon,
    epsilon_decay=epsilon_decay,
    final_epsilon=final_epsilon,
)

"""# **Training the Q-learning Agent**

In this section, we train the Blackjack agent using the Q-learning algorithm. 
Over 600.000 episodes, the agent interacts with the environment, updates its Q-values, and adjusts its exploration rate. 
We also record basic statistics like wins, losses, and draws. The training loop makes the agent progressively improve its strategy through trial and error.
"""

# Wrap the environment to track statistics like rewards and episode lengths
env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)

# Initialize counters for win/loss/draw statistics
n_wins = 0
n_losses = 0
n_draws = 0

epsilon_history = []

for episode in tqdm(range(n_episodes), desc="Training", leave=False):

    # Start a new episode: reset environment and get initial observation
    obs, _ = env.reset()
    done = False

    # Play one episode until it ends
    while not done:
        # Select action based on epsilon-greedy policy
        action = agent.get_action(env, obs)
        # Apply action and observe next state and reward
        next_obs, reward, terminated, truncated, _ = env.step(action)
        # Update the Q-table using the observed transition
        agent.update(obs, action, reward, terminated, next_obs)
        # Check if episode is done (natural end or truncated)
        done = terminated or truncated
        # Move to the next state
        obs = next_obs

    # After the episode: extract player state and dealer's card
    player_score = obs[0]
    dealer_card = obs[1]
    has_usable_ace = obs[2]

    # Update outcome counters based on final reward
    match reward:
        case 1:
            outcome = "Win"
            n_wins += 1
        case -1:
            outcome = "Lose"
            n_losses += 1
        case _:
            outcome = "Draw"
            n_draws += 1

    # Decay epsilon to reduce exploration over time
    agent.decay_epsilon()
    epsilon_history.append(agent.epsilon)


# Print final training results
print("\n--- Final results after training ---")
print(f"Total wins:     {n_wins} âœ…")
print(f"Total losses:   {n_losses} âŒ ")
print(f"Total draws:    {n_draws} ðŸ¤")
print("\n")

# Plot Epsilon Decay
plt.figure(figsize=(8, 4))
plt.plot(epsilon_history)
plt.title("Epsilon Decay")
plt.xlabel("Episode")
plt.ylabel("Epsilon (exploration rate)")
plt.grid(True)
plt.show()

print("\n--- Q-table ---")
for state, action_values in agent.q_values.items():
    best_action = np.argmax(action_values)  # 0 = stick, 1 = hit
    print(f"State: {state} â†’ Q-values: {action_values} â†’ Best action: {best_action} ({'stick' if best_action == 0 else 'hit'})")

"""# **Visualizing the Training**

In this section, we visualize the training progress of our Blackjack Q-learning agent. We generate three plots:

*   **Episode rewards**: how the average reward evolves over time
*   **Episode lengths**: how many steps are played per episode
*   **Training error**: the temporal difference (TD) error, indicating learning progress
"""

# Number of episodes over which to compute moving average
rolling_length = 500

fig, axs = plt.subplots(nrows=3, figsize=(20, 15))

# ============================
# 1. Plot: Average reward per episode
# ============================

axs[0].set_title("Episode rewards", fontsize=24)
reward_moving_average = (
    np.convolve(
        np.array(env.return_queue).flatten(),
        np.ones(rolling_length),
        mode="valid"
    ) / rolling_length
)
axs[0].plot(range(len(reward_moving_average)), reward_moving_average)
axs[0].axhline(0, color='red', linestyle='--', linewidth=1)

# ============================
# 2. Plot: Episode lengths
# ============================

axs[1].set_title("Episode lengths", fontsize=24)
length_moving_average = (
    np.convolve(
        np.array(env.length_queue).flatten(),
        np.ones(rolling_length),
        mode="same"
    ) / rolling_length
)
axs[1].plot(range(len(length_moving_average)), length_moving_average)

# ============================
# 3. Plot: Temporal Difference (TD) Error
# ============================

axs[2].set_title("Training Error", fontsize=24)
training_error_moving_average = (
    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode="same")
    / rolling_length
)
axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)
axs[2].axhline(0, color='red', linestyle='--', linewidth=1)


for ax in axs:
    ax.tick_params(axis='both', labelsize=18)

plt.tight_layout()
plt.show()

"""#**Visualizing the Policy**

In this section, we visualize the final learned value function and the policy of the agent after training.
The plots show what the agent has learned for all possible game states, separately for cases where the player has or does not have a usable ace.

*   The value plot (3D) shows how good each state is, in terms of expected reward.
*   The policy heatmap shows whether the agent chooses to hit (green) or stick (gray) in each situation.
"""

def create_grids(agent, usable_ace=False):

    # Generate the state-value and policy grids for plotting.
    # These represent what the agent has learned, for all combinations of:
    # - player total (12â€“21)
    # - dealer's visible card (1â€“10)
    # - whether the player has a usable ace

    state_value = defaultdict(float) # Best Q-value for each state
    policy = defaultdict(int) # Best action for each state

    # Extract values and actions from the Q-table learned by the agent
    for obs, action_values in agent.q_values.items():
        state_value[obs] = float(np.max(action_values))
        policy[obs] = int(np.argmax(action_values))

    # Generate all combinations of player totals and dealer cards
    player_count, dealer_count = np.meshgrid(
        np.arange(12, 22),
        np.arange(1, 11),
    )

    # Value grid: map each (player, dealer) pair to its state value
    value = np.apply_along_axis(
        lambda obs: state_value[(obs[0], obs[1], usable_ace)],
        axis=2,
        arr=np.dstack([player_count, dealer_count]),
    )
    value_grid = player_count, dealer_count, value

    # Policy grid: map each (player, dealer) pair to the selected action
    policy_grid = np.apply_along_axis(
        lambda obs: policy[(obs[0], obs[1], usable_ace)],
        axis=2,
        arr=np.dstack([player_count, dealer_count]),
    )
    return value_grid, policy_grid


def create_plots(value_grid, policy_grid, title: str):

    # Create two plots side-by-side:
    # - A 3D surface plot of the state-value function
    # - A 2D heatmap showing the selected action (policy)

    player_count, dealer_count, value = value_grid
    fig = plt.figure(figsize=plt.figaspect(0.4))
    fig.suptitle(title, fontsize=16)

    # --- Left: 3D value function ---
    ax1 = fig.add_subplot(1, 2, 1, projection="3d")
    ax1.plot_surface(
        player_count,
        dealer_count,
        value,
        rstride=1,
        cstride=1,
        cmap="viridis",
        edgecolor="none",
    )
    plt.xticks(range(12, 22), range(12, 22))
    plt.yticks(range(1, 11), ["A"] + list(range(2, 11)))
    ax1.set_title(f"State values: {title}")
    ax1.set_xlabel("Player sum")
    ax1.set_ylabel("Dealer showing")
    ax1.zaxis.set_rotate_label(False)
    ax1.set_zlabel("Value", fontsize=14, rotation=90)
    ax1.view_init(20, 220)

    # --- Right: 2D policy heatmap ---
    fig.add_subplot(1, 2, 2)
    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap="Accent_r", cbar=False)
    ax2.set_title(f"Policy: {title}")
    ax2.set_xlabel("Player sum")
    ax2.set_ylabel("Dealer showing")
    ax2.set_xticklabels(range(12, 22))
    ax2.set_yticklabels(["A"] + list(range(2, 11)), fontsize=12)

    # Add legend for hit/stick
    legend_elements = [
        Patch(facecolor="lightgreen", edgecolor="black", label="Hit"),
        Patch(facecolor="grey", edgecolor="black", label="Stick"),
    ]
    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))
    return fig


# Value and policy for the case where the agent has a usable ace (Ace counts as 11)
value_grid, policy_grid = create_grids(agent, usable_ace=True)
fig1 = create_plots(value_grid, policy_grid, title="With usable ace")
plt.show()

# Value and policy for the case where the agent does NOT have a usable ace (Ace counts as 1)
value_grid, policy_grid = create_grids(agent, usable_ace=False)
fig2 = create_plots(value_grid, policy_grid, title="Without usable ace")
plt.show()

"""# **Testing**

After training, we evaluate the agent by running it for a fixed number of episodes with no exploration (Îµ = 0). 
This means the agent always selects the best-known action based on the learned Q-table. 
During this test phase, no learning or Q-value updates are performed, we simply measure how well the policy performs.
"""

test_episodes = 10000
test_wins = 0
test_losses = 0
test_draws = 0

# Set epsilon to 0 â†’ agent always exploits the best-known action
agent.epsilon = 0.0

for _ in tqdm(range(test_episodes), desc="Testing", leave=False):
    obs, _ = env.reset()
    done = False

    while not done:
        # Select the best action
        action = agent.get_action(env, obs)
        # Apply the action in the environment
        next_obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        obs = next_obs

    if reward == 1:
        test_wins += 1
    elif reward == -1:
        test_losses += 1
    else:
        test_draws += 1

# Print test performance summary
print("\n--- Test results ---")
print(f"Wins:   {test_wins} âœ…")
print(f"Losses: {test_losses} âŒ")
print(f"Draws:  {test_draws}  ðŸ¤")
print(f"Win rate: {test_wins / test_episodes:.2%} ðŸ† \n")


# === Bar chart of test outcomes ===
labels = ["Wins", "Losses", "Draws"]
counts = [test_wins, test_losses, test_draws]
colors = ["green", "red", "gray"]

plt.figure(figsize=(6, 4))
plt.bar(labels, counts, color=colors)
plt.title("Test Results")
plt.ylabel("Number of Episodes")
plt.show()